{"cells": [{"cell_type": "markdown", "id": "1ee373c5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 15: Deep Learning Regression</h1>\n"]}, {"cell_type": "markdown", "id": "41345f3a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_15_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L15.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "fce9b388", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_15_1\">L15.1 Discovering the Higgs with Deep Learning</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_15_1\">L15.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_15_2\">L15.2 Minimizing Loss with a Neural Network</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_15_2\">L15.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_15_3\">L15.3 An Example with PyTorch: Fitting a Parabola</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_15_3\">L15.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_15_4\">L15.4 Another Example: Fitting a Sine Function</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_15_4\">L15.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_15_5\">L15.5 Sine Function Continued: Adjusting the Network</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_15_5\">L15.5 Exercises</a></td>\n", "    </tr>\n", "</table>"]}, {"cell_type": "markdown", "id": "467638f0", "metadata": {"tags": ["learner", "catsoop_00", "md"]}, "source": ["<h3>Learning Objectives</h3>\n", "\n", "It is often the case that you want to fit a function to data. However, sometimes you want to do this in many dimensions and you can't visualize it all. In this lecture, we are going to look at deep learning regression. This can help you to model these complex scenarios. \n", "\n", "Let's say you have some data $\\vec{x}$, this can be an n-dimensional set of inputs, and you want to predict an output $\\vec{y}$ from $\\vec{x}$ where $y$ can be an m-dimensional set of outputs. What we want then is to create a function\n", "\n", "$$\n", "\\begin{equation}\n", " \\vec{y} = f(\\vec{x})\n", "\\end{equation}\n", "$$\n", "\n", "Earlier in this course,  we did this for a one dimensional $y$ taking in a set of inputs $\\vec{x}$. In this Lesson, we aim to generalize this to predict an arbitrary number of outputs with an arbitrary set of inputs. In particular, we will explore the following learning objectives:\n", "\n", " - Fitting an arbitrary 1D dataset with a neural net\n", " - Deep Learning algorithm design\n", " - Studying decays of Higgs bosons to Tau leptons\n", " - Observing the improvements\n", " - Optimized target\n", " - The full mass regression\n", " - NN architecture\n"]}, {"cell_type": "markdown", "id": "21325e06", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Installing Tools</h3>\n", "\n", "Before we do anything, lets make sure we install the tools we need for this."]}, {"cell_type": "code", "execution_count": null, "id": "d59f3ad0", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.0-runcell00\n", "\n", "!pip install torch\n", "!pip install imageio\n", "!pip install awkward\n", "!pip install george\n", "!pip install uproot\n", "!pip install pylorentz"]}, {"cell_type": "markdown", "id": "dff0575f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Importing Libraries</h3>\n", "\n", "Before beginning, run the cell below to import the relevant libraries for this notebook. "]}, {"cell_type": "code", "execution_count": null, "id": "0f3a0154", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.0-runcell01\n", "\n", "import torch                        #https://pytorch.org/docs/stable/torch.html\n", "import torch.nn as nn               #https://pytorch.org/docs/stable/nn.html\n", "from torch.autograd import Variable #https://pytorch.org/docs/stable/autograd.html\n", "import torch.nn.functional as F     #https://pytorch.org/docs/stable/nn.functional.html\n", "import torch.utils.data as Data     #https://pytorch.org/docs/stable/data.html\n", "\n", "import matplotlib.pyplot as plt     #https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html\n", "%matplotlib inline\n", "\n", "import numpy as np                  #https://numpy.org/doc/stable/\n", "import imageio                      #https://imageio.readthedocs.io/en/stable/\n", "import george                       #https://george.readthedocs.io/en/latest/\n", "from george import kernels          #https://george.readthedocs.io/en/latest/user/kernels/"]}, {"cell_type": "markdown", "id": "3d515076", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Setting Default Figure Parameters</h3>\n", "\n", "The following code cell sets default values for figure parameters."]}, {"cell_type": "code", "execution_count": null, "id": "97d0edf8", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.0-runcell02\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title"]}, {"cell_type": "markdown", "id": "1f186cb0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_15_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L15.1 Discovering the Higgs with Deep Learning</h2>  \n", "\n", "| [Top](#section_15_0) | [Previous Section](#section_15_0) | [Exercises](#exercises_15_1) | [Next Section](#section_15_2) |\n"]}, {"cell_type": "markdown", "id": "990af1fa", "metadata": {"tags": ["learner", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.2x+1T2025/block-v1:MITxT+8.S50.2x+1T2025+type@sequential+block@seq_LS15/block-v1:MITxT+8.S50.2x+1T2025+type@vertical+block@vert_LS15_vid1\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "ff9f401f", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L19/slides_L19_01.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "9d3798ed", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.1-slides\n", "\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L19/slides_L19_01.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "491c85e8", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_15_1'></a>     \n", "\n", "| [Top](#section_15_0) | [Restart Section](#section_15_1) | [Next Section](#section_15_2) |\n"]}, {"cell_type": "markdown", "id": "eb71daaa", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.1.1</span>\n", "\n", "Look again at the Higgs discovery plots. At the Higgs mass, the CMS experiment is about 30% more sensitive than ATLAS, but both experiments found approximately the same excess. This is because:\n", "\n", "A) The ATLAS experiment was less sensitive, but had more data.\\\n", "B) The ATLAS experiment had more advanced machine learning analysis tools.\\\n", "C) This was just a random fluctuation, and not a particularly unlikely one.\\\n", "D) The CMS data was noisier."]}, {"cell_type": "markdown", "id": "e446de2e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_15_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L15.2 Minimizing Loss with a Neural Network</h2>  \n", "\n", "| [Top](#section_15_0) | [Previous Section](#section_15_1) | [Exercises](#exercises_15_2) | [Next Section](#section_15_3) |\n"]}, {"cell_type": "markdown", "id": "1fe3f30b", "metadata": {"tags": ["learner", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.2x+1T2025/block-v1:MITxT+8.S50.2x+1T2025+type@sequential+block@seq_LS15/block-v1:MITxT+8.S50.2x+1T2025+type@vertical+block@vert_LS15_vid2\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "7e429584", "metadata": {"tags": ["learner", "md"]}, "source": ["<h3>Slides</h3>\n", "\n", "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L19/slides_L19_02.html\" target=\"_blank\">HERE</a>."]}, {"cell_type": "code", "execution_count": null, "id": "9fef113b", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.2-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L19/slides_L19_02.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "a7430bef", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["<h3>Fitting an arbitrary 1D dataset with a neural net</h3>\n", "\n", "What we would like to do is fit a distribution without an initial choice of a function. To envision this distribution, let's create a Gaussian function that is smeared out a little bit. From that, we can try to fit this dataset to get a functional form for this. "]}, {"cell_type": "code", "execution_count": null, "id": "83bf6d2b", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.2-runcell01\n", "\n", "#Let's Try GP on a Gaussian random set of points\n", "def gaussian(mu,sigma,norm,offset):\n", "    \"\"\"Returns a gaussian function with the given parameters\"\"\"\n", "    return lambda x: norm*np.exp(-(1./2.)*((x-mu)/(sigma))**2)+offset\n", "\n", "Xin = np.mgrid[0:201] # points from 0 to 201\n", "data = gaussian(100., 20., 10., 5)(Xin) + 5*np.random.random(Xin.shape) # Guassian + semaring\n", "plt.plot(data,\"*\")\n", "plt.show()"]}, {"cell_type": "markdown", "id": "5d340246", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["Now, one way to model this distribution is through a Gaussian process. This will try to fit very many Gaussians that will allow us to extract a function. "]}, {"cell_type": "code", "execution_count": null, "id": "9f394406", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.2-runcell02\n", "\n", "#now let's run GP on this guy\n", "\n", "kernel = np.var(data) * kernels.ExpSquaredKernel(1.5)\n", "gp = george.GP(kernel)\n", "var=np.ones(len(Xin))\n", "gp.compute(Xin,var)\n", "x_pred = np.linspace(0, 200, 100)\n", "pred, pred_var = gp.predict(data, x_pred, return_var=True)\n", "\n", "plt.fill_between(x_pred, pred - np.sqrt(pred_var), pred + np.sqrt(pred_var),color=\"k\", alpha=0.2)\n", "plt.plot(x_pred, pred, \"k\", lw=1.5, alpha=0.5)\n", "plt.plot(Xin,data,\"*\")\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"y\");"]}, {"cell_type": "markdown", "id": "5492c50c", "metadata": {"tags": ["learner", "md", "lect_02"]}, "source": ["There are lots of fluctuations here. Clearly, the function is not ideal, and with a bit of tuning and smoothening, we could probably better approximate the underlying function. However, another way we can do this is with a neural network. "]}, {"cell_type": "markdown", "id": "9aeb48c9", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_15_2'></a>     \n", "\n", "| [Top](#section_15_0) | [Restart Section](#section_15_2) | [Next Section](#section_15_3) |\n"]}, {"cell_type": "markdown", "id": "9ec4af2f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.2.1</span>\n", "\n", "In the last lesson, when we applied a neural network to separate two samples, we cared only about minimizing the binary cross entropy. We did not care about knowing where the points are distributed. When we fit a function, we need to guess a form to fit the data and minimize $\\chi^2$. Do we need to do that for a neural network?  \n", "\n", "A) The neural network needs an architecture that is similar to the functional guess.\\\n", "B) If there are sufficient parameters, the neural network can approximate the function, no matter the form.\\\n", "C) Specific architectures are needed for specific problems (CNNs for images, RNNs for time series).\\\n", "D) The neural network needs to work with our statistical tools like Gaussian processes and f-tests to get the right function."]}, {"cell_type": "markdown", "id": "02b3a423", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.2.2</span>\n", "\n", "In the preceding code cell, `L15.2-runcell02`, we used the Gaussian Process regression algorithm to fit a curve to some data. Explore using different metrics in the function <a href=\"https://dfm.io/george/dev/user/kernels/#george.kernels.ExpSquaredKernel\" target=\"_blank\">`george.kernels.ExpSquaredKernel`</a> (currently, the default is set to 1.5). The code cell is repeated below, in the notebook. \n", "\n", "What approximate value of the metric should one use to smooth out the extraneous wiggles in the plot?\n", "\n", "A) 0.5\\\n", "B) 2.5\\\n", "C) 5\\\n", "D) 20\\\n", "E) 100"]}, {"cell_type": "code", "execution_count": null, "id": "4d698bca", "metadata": {"tags": ["py", "learner_chopped", "draft"]}, "outputs": [], "source": ["#>>>EXERCISE: L15.2.2\n", "\n", "#now let's run GP on this guy\n", "\n", "kernel = np.var(data) * kernels.ExpSquaredKernel(1.5)\n", "gp = george.GP(kernel)\n", "var=np.ones(len(Xin))\n", "gp.compute(Xin,var)\n", "x_pred = np.linspace(0, 200, 100)\n", "pred, pred_var = gp.predict(data, x_pred, return_var=True)\n", "\n", "plt.fill_between(x_pred, pred - np.sqrt(pred_var), pred + np.sqrt(pred_var),color=\"k\", alpha=0.2)\n", "plt.plot(x_pred, pred, \"k\", lw=1.5, alpha=0.5)\n", "plt.plot(Xin,data,\"*\")\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"y\");"]}, {"cell_type": "markdown", "id": "1927751b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_15_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L15.3 An Example with PyTorch: Fitting a Parabola</h2>  \n", "\n", "| [Top](#section_15_0) | [Previous Section](#section_15_2) | [Exercises](#exercises_15_3) | [Next Section](#section_15_4) |\n"]}, {"cell_type": "markdown", "id": "6d0fb911", "metadata": {"tags": ["learner", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.2x+1T2025/block-v1:MITxT+8.S50.2x+1T2025+type@sequential+block@seq_LS15/block-v1:MITxT+8.S50.2x+1T2025+type@vertical+block@vert_LS15_vid3\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "d09219b7", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["<h3>Overview</h3>\n", "\n", "In previous setups for classification, we defined a loss function as:\n", "\n", "$$\n", "\\begin{equation}\n", "\\mathcal{L} = y \\log(f(x)) + (1-y)\\log(1-f(x))\n", "\\end{equation}\n", "$$\n", "\n", "where $f(x)$ is our classifier with 1 being a high likelihood of a signal, and $0$ denoting a high likelihood of a background. Note that the goal here is to minimize the loss, so we want to make sure that f(x) and 1-f(x) are orthogonal.  \n", "\n", "When we are running a deep learning algorithm, what we are effectively doing is to minimize the parameters: \n", "\n", "$$\n", "\\begin{equation}\n", "\\frac{\\partial \\mathcal{L}}{\\partial w_{i}}\\rightarrow 0\n", "\\end{equation}\n", "$$\n", "\n", "Now, the power of deep learning is that we can fit any arbitrary function to the data. \n", "\n", "Let's start with a simplified dataset. Before we go and fit the noisy Gaussian we generated previously, let's just try to fit $y=x^{2}$, again with added noise. I know this is kind of silly, but it will help with your understanding of how deep learning works. \n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "f60bceb6", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.3-runcell01\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)\n", "y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)\n", "\n", "# torch can only train on Variable, so convert them to Variable\n", "x, y = Variable(x), Variable(y)\n", "\n", "# view data\n", "plt.figure(figsize=(10,4))\n", "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "plt.title('Regression Analysis')\n", "plt.xlabel('Independent variable')\n", "plt.ylabel('Dependent variable')\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "2448a675", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["<h3>Mean Squared Error Loss</h3>\n", "\n", "Now, to fit this function, we can imagine defining a new loss that we can minimize. In this case, we will define a loss known as mean squared error (MSE) loss. This loss can be written as \n", "\n", "$$\n", "\\begin{equation}\n", "\\mathcal{L} = \\left(y-f(x)\\right)^{2}\n", "\\end{equation}\n", "$$\n", "\n", "More generally, we can write this over $N$ variables as \n", "\n", "$$\n", "\\begin{equation}\n", "\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N}\\left(y_{i}-f(x_{i})\\right)^{2}\n", "\\end{equation}\n", "$$\n", "\n", "There are several variations on this loss. Perhaps the most common one is known as mean absolute percentage error (MAPE). This loss is defined as \n", "\n", "$$\n", "\\begin{equation}\n", "\\mathcal{L} = \\frac{100\\%}{N}\\sum_{i=1}^{N}\\frac{|y_{i}-f(x_{i})|}{y_{i}}\n", "\\end{equation}\n", "$$"]}, {"cell_type": "markdown", "id": "e088100c", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["<h3>Defining a two-layered network</h3>\n", "\n", "Let's fit the noisy parabola data using deep learning regression. To do this, we are going train a 2-layered dense network to predict this data. \n", "\n", "Our 2-layered model can be written as \n", "\n", "$$\n", "\\begin{equation}\n", " f(x) = W^{T}_{2}\\left(\\rm{Act}\\left(\\vec{W_{1}}x + \\vec{b}\\right)+b\\right)\n", "\\end{equation}\n", "$$\n", "\n", "where $W_{1}$ and $W_{2}$ would generally denote matrices. However, in this particular case, they are really vectors since the output dimension is 1 and the input dimension is 1. With these matrices, we can specify the number of hidden parameters, which will be the size of the alternative dimension. For 10 hidden parameters, $W_{i}$ will be a 10x1 matrix(a 10 dimensional vector). The symbol $\\rm{Act}$ denotes the activation function for this system. \n", "\n", "Let's go ahead and build the neural network with 10 hidden parameters, and use a mean squared loss. Additionally, for the minimizer, we will use stochastic gradient descent (SGD). "]}, {"cell_type": "code", "execution_count": null, "id": "ac9b50bd", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.3-runcell02\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "# this is one way to define a network\n", "class Net(torch.nn.Module):\n", "    def __init__(self, n_feature, n_hidden, n_output):\n", "        super(Net, self).__init__()\n", "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n", "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n", "\n", "    def forward(self, x):\n", "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n", "        x = self.predict(x)             # linear output\n", "        return x\n", "\n", "net = Net(n_feature=1, n_hidden=10, n_output=1)     # define the network\n", "print(net)  # net architecture\n", "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)#stochastic Gradient Descent\n", "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss"]}, {"cell_type": "markdown", "id": "50b9ee8c", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["<h3>Network Training</h3>\n", "\n", "Ok, now that we have our neural network, what we are going to do is run 200 epochs of the network training, and for each epoch, we are going to make a plot. Then we will turn these plots into a moving video to see how the network is running. \n", "\n", "**NOTE:** If you run the next code cell multiple times, the network will continue to train from where it previously finished. Therefore, to restart the training process (and see the animated .gif from its initial flat-line state), you must redefine and reinitialize the network by running code cell `L15.3-runcell02` first.\n", "\n", "One interesting feature of this code is that the training actually runs quite quickly, but generating the animated gif with `imageio.mimsave` takes a fair amount of time.\n"]}, {"cell_type": "code", "execution_count": null, "id": "29052969", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.3-runcell03\n", "\n", "def makePlot(x,y,prediction,ax,fig,images,t,loss,ymin,ymax):\n", "    # plot and show learning process\n", "    plt.cla()\n", "    ax.set_title('Regression Analysis', fontsize=35)\n", "    ax.set_xlabel('Independent variable', fontsize=24)\n", "    ax.set_ylabel('Dependent variable', fontsize=24)\n", "    ax.set_ylim(ymin,ymax)\n", "    ax.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "    ax.plot(x.data.numpy(), prediction.data.numpy(), 'g-', lw=3)\n", "    ax.text(0.6, 0.7, 'Epoch = %d' % t, fontdict={'size': 24, 'color':  'red'})\n", "    ax.text(0.6, 0.3, 'Loss = %.4f' % loss.data.numpy(),fontdict={'size': 24, 'color':  'red'}) \n", "    fig.canvas.draw()       # draw the canvas, cache the renderer\n", "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n", "    image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n", "    images.append(image)\n", "\n", "def train(x,y,net,loss_func,opt,nepochs,ymin,ymax):\n", "    images = []\n", "    fig, ax = plt.subplots(figsize=(12,7))\n", "    for epoch in range(nepochs):\n", "        if epoch % 50 == 0: \n", "            print(\"epoch:\",epoch)\n", "        prediction = net(x)\n", "        loss = loss_func(prediction, y) \n", "        opt.zero_grad()\n", "        loss.backward() \n", "        optimizer.step()\n", "        nplots = int(nepochs/40)\n", "        if epoch % nplots == 0:\n", "            makePlot(x,y,prediction,ax,fig,images,epoch,loss,ymin,ymax)\n", "    return images\n", "    \n", "from IPython.display import Image\n", "images=train(x,y,net,loss_func,optimizer,200,-0.1,1.5)\n", "imageio.mimsave('data/L15/curve_1.gif', images, fps=10)\n", "Image(open('data/L15/curve_1.gif','rb').read())"]}, {"cell_type": "markdown", "id": "d69b1c75", "metadata": {"tags": ["learner", "md", "lect_03"]}, "source": ["Try exploring more epochs by running code cell `L15.3-runcell03` multiple times without reinitializing the network. You will notice that the network seems to come up with an output that looks like a set of connected lines. Why do you think that happens?\n", "\n", "So, this is actually a great way to play around with models. You quickly see what the neural network is trying to do, and its output sort of approximates what you might get from fitting. You may also notice that this is a lot slower than fitting. Do you understand why? \n", "\n", "The answer involves the fact that the minimization procedure we are doing is not a full fit procedure starting from a fixed functional form. We are computing a gradient, but we are doing a \"soft\" propagation. This is not the full stepping and fitting that we did with our original fits. The advantage, of course, is that we have many more parameters now to play with. \n", "\n", "For a quick look at how things change, let's double the number of hidden parameters. What we want to do is rerun code cell `L15.3-runcell02` to define a neural network with 20 neurons in the hidden layer. Then, we can run code cell `L15.3-runcell03` to see how the output evolves with epoch using this larger hidden layer."]}, {"cell_type": "markdown", "id": "0c9f00b3", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_15_3'></a>     \n", "\n", "| [Top](#section_15_0) | [Restart Section](#section_15_3) | [Next Section](#section_15_4) |\n"]}, {"cell_type": "markdown", "id": "64d0bc45", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.3.1</span>\n", "\n", "What are the dimensions of the input, output, and hidden layers? Enter your answer as a list of integers: `[input, output, hidden]`"]}, {"cell_type": "markdown", "id": "7d88eff8", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.3.2</span>\n", "\n", "Why do we choose to use mean squared error for our loss function? Select all the apply:\n", "\n", "A) This is the same as our original fit minimization (least squares).\\\n", "B) We did not give uncertainties, so we cannot do chi2.\\\n", "C) Mean squared error is the closest we get to absolute value.\\\n", "D) Actually, we should not use a NN at all, and we really should do a full systematic approach of adding polynomials and fitting to determine the functional form."]}, {"cell_type": "markdown", "id": "f630da0f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.3.3</span>\n", "\n", "You saw that increasing the number of hidden parameters (i.e. the number of neurons in the hidden layer) gave a better by-eye fit. Why not use as many hidden parameters as possible? Wouldn't this make an even better fit to the data? Why would it NOT be ideal to arbitrarily add more hidden parameters? Select all that apply:\n", "\n", "A) It can lead to overfitting of the model to the training data.\\\n", "B) It can make the model too simple and unable to learn complex features.\\\n", "C) It can require more computational resources.\\\n", "D) It can decrease the accuracy of the model on the training data.\\\n", "E) There is an ideal number of hidden parameters that works universally well for all neural networks.\n"]}, {"cell_type": "markdown", "id": "be2f50d3", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_15_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L15.4 Another Example: Fitting a Sine Function</h2>  \n", "\n", "| [Top](#section_15_0) | [Previous Section](#section_15_3) | [Exercises](#exercises_15_4) | [Next Section](#section_15_5) |\n"]}, {"cell_type": "markdown", "id": "3348a7aa", "metadata": {"tags": ["learner", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.2x+1T2025/block-v1:MITxT+8.S50.2x+1T2025+type@sequential+block@seq_LS15/block-v1:MITxT+8.S50.2x+1T2025+type@vertical+block@vert_LS15_vid4\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "aa1a3b10", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["<h3>Overview</h3>\n", "\n", "Ok, now let's look at another function to regress, let's try to regress \n", "\n", "\\begin{equation}\n", " f(x) = \\sin(x)\n", "\\end{equation}"]}, {"cell_type": "code", "execution_count": null, "id": "c6069574", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.4-runcell01\n", "\n", "torch.manual_seed(1)    # reproducible\n", "x = torch.unsqueeze(torch.linspace(-10, 10, 1000), dim=1)  # x data (tensor), shape=(100, 1)\n", "y = torch.sin(x) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)\n", "\n", "# torch can only train on Variable, so convert them to Variable\n", "x, y = Variable(x), Variable(y)\n", "plt.figure(figsize=(10,4))\n", "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "plt.title('Regression Analysis')\n", "plt.xlabel('Independent variable')\n", "plt.ylabel('Dependent variable')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "2bcc6fd8", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["<h3>Challenge question</h3>\n", "\n", "What happens if we make a regression for the above dataset? How does this regression change with the number of parameters, say 100 hidden parameters?"]}, {"cell_type": "code", "execution_count": null, "id": "d0ceaf00", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.4-runcell02\n", "\n", "#redefine network\n", "net = Net(n_feature=1, n_hidden=10, n_output=1)     # define the network, try changing to n_hidden=100\n", "#net = Net(n_feature=1, n_hidden=100, n_output=1)\n", "\n", "images=train(x,y,net,loss_func,optimizer,200,-1.1,1.1)\n", "imageio.mimsave('data/L15/curve_2.gif', images, fps=10)\n", "Image(open('data/L15/curve_2.gif','rb').read())"]}, {"cell_type": "markdown", "id": "d65c479d", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["This fit is doing an astoundingly bad job! You can see that it's not reproducing any features of the data at all, even though we have 100 parameters. Would more epochs help? Try rerunning code cell L15.4-runcell02 a few times to check.\n", "\n", "In the next section, we'll to understand more about why this attempt fails so miserably.        "]}, {"cell_type": "markdown", "id": "cb7a8563", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["<h3>Deep Learning Algorithm Design</h3>\n", "\n", "Now, we want to build some intuition about how neutral networks work. Let's take the above problem and see if we can really explain its behavior by doing some deep learning R&D. To do that, let's first take an architecture similar to the last one, except with more than 2 layers. \n", "\n", "Let's start with a 3-layer network with 100 hidden parameters in both the first and second layers.  "]}, {"cell_type": "code", "execution_count": null, "id": "298a29ea", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.4-runcell03\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "# another way to define a network\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.Linear(100, 100),\n", "        torch.nn.Linear(100, 1),\n", "    )\n", "print(net[0].weight[0:10])\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,200,-1.1,1.1)\n", "imageio.mimsave('data/L15/curve_2.gif', images, fps=10)\n", "Image(open('data/L15/curve_2.gif','rb').read())"]}, {"cell_type": "markdown", "id": "274d9e1b", "metadata": {"tags": ["learner", "md", "lect_04"]}, "source": ["Yet again, we see a total failure to match any aspect of the data! Why did the neural network just try to fit a line in this case? \n", "\n", "The answer is a bit subtle, but the network lacks enough \"expressiveness\" to solve this particular problem. The linear layers that we used correspond to a simple matrix multiplication. That means that each layer just multiplies its input by a constant and also adds an offset $ax+b$. Now, it's true that $a$ can be a matrix. However, all that does is give a vector of linear outputs.\n", "\n", "Note that this explains why the previous attempt to fit a noisy parabola resulted in a fit that was a sequence of connected short line segments.\n", "\n", "To fix this mismatch between the network architecture and the data we are trying to fit, we will add a different sort of \"activation\" layer between the linear layers. This will be covered in the next section. "]}, {"cell_type": "markdown", "id": "e0f21fb0", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_15_4'></a>     \n", "\n", "| [Top](#section_15_0) | [Restart Section](#section_15_4) | [Next Section](#section_15_5) |\n"]}, {"cell_type": "markdown", "id": "086d2035", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.4.1</span>\n", "\n", "Which of the following statements describes why linear activation function layers are not suitable for classification of data which is strongly nonlinear?\n", "\n", "A) A linear activation function cannot capture complex nonlinear relationships between the input features and the output classes.\\\n", "B) A linear activation function is only effective for classification tasks with linearly separable data.\\\n", "C) A linear activation function is prone to underfitting and can lead to poor performance on the training and test sets.\\\n", "D) A linear activation function can introduce too much noise into the model and reduce its accuracy."]}, {"cell_type": "markdown", "id": "c3c9cac5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_15_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L15.5 Sine Function Continued: Adjusting the Network</h2>  \n", "\n", "| [Top](#section_15_0) | [Previous Section](#section_15_4) | [Exercises](#exercises_15_5) |"]}, {"cell_type": "markdown", "id": "e3310dce", "metadata": {"tags": ["learner", "md"]}, "source": ["*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.2x+1T2025/block-v1:MITxT+8.S50.2x+1T2025+type@sequential+block@seq_LS15/block-v1:MITxT+8.S50.2x+1T2025+type@vertical+block@vert_LS15_vid5\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"]}, {"cell_type": "markdown", "id": "c4a6f6f1", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Overview</h3>\n", "\n", "Now, let's build on the intuition we started to gain about how neutral networks work. Let's take the previous problem of fitting a noisy sine wave and see if we can really explain its behavior by doing some deep learning R&D. To do that, let's first take an architecture similar to the last one, except with one addition. \n", "\n", "Let's start with a 3-layer network with 100 and 50 hidden parameters in the first and second layers, respectively. However, we will now add one ReLU activation function into the network. We will skip right down to the point where we can train it. "]}, {"cell_type": "code", "execution_count": null, "id": "437687d2", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell01\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "# another way to define a network\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,200,-1.1,1.1)\n", "imageio.mimsave('data/L15/curve_2.gif', images, fps=10)\n", "Image(open('data/L15/curve_2.gif','rb').read())"]}, {"cell_type": "markdown", "id": "4d490f65", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["What you see is that the ReLU activation function introduced a \"kink\" in the final fit. The result is still pretty awful, but it does seem to be adding a new feature that is trending in the right direction. Let's keep going and add a second ReLU activation function. Also, let's increase the number of epochs to give the network more time to adjust the parameters."]}, {"cell_type": "code", "execution_count": null, "id": "76a868ed", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell02\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "# another way to define a network\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,400,-1.1,1.1)\n", "imageio.mimsave('data/L15/curve_2.gif', images, fps=10)\n", "Image(open('data/L15/curve_2.gif','rb').read())"]}, {"cell_type": "markdown", "id": "ac9f3c52", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["OK, now the fit is improving dramatically, at least in the central region. Looking closely, you can see that the green line is again a series of connected straight lines. However, the \"kinks\" introduced by the ReLU functions allow the network the flexibility (what we have also called \"expressiveness\") to deal with the strong non-linearities in the data."]}, {"cell_type": "markdown", "id": "8660efdc", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["<h3>Challenge Question</h3>\n", "\n", "Given the observed trends, add more layers to your network. How many are needed to describe the whole oscillation? Would a third pair of Linear layer plus ReLU activation function be enough?"]}, {"cell_type": "code", "execution_count": null, "id": "3002338f", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell03\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,400,-1.1,1.1)\n", "imageio.mimsave('data/L15/curve_2.gif', images, fps=10)\n", "Image(open('data/L15/curve_2.gif','rb').read())"]}, {"cell_type": "markdown", "id": "2b67f869", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["How about 4?"]}, {"cell_type": "code", "execution_count": null, "id": "6fd24f06", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell04\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,400,-1.1,1.1)\n", "imageio.mimsave('data/L15/curve_2.gif', images, fps=10)\n", "Image(open('data/L15/curve_2.gif','rb').read())"]}, {"cell_type": "markdown", "id": "5a56b43c", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["So, something in the range of 3-4 ReLU functions seems to do the trick. This makes sense since the range of the sine wave that we tried to fit looks something like 3 (or maybe 3.5) parabolas, but you need the ReLU kinks between them to get it all to line up.\n", "\n", "You may have noticed that the network with 2 ReLU functions did a sort of reasonable job of fitting 2 of the sine wave peaks, but with a single ReLU function the output didn't seem to match anything at all. One reason for this difference is that it's almost impossible to fit only one sine wave peak without having the rest of the output wildly different from the data.\n", "\n", "Now that we have all of this working, what would be the right architecture to perform a regression on the noisy Gaussian we started out with? \n", "\n", "Let's setup the data using the torch tools."]}, {"cell_type": "code", "execution_count": null, "id": "9da1e90e", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell05\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "x=torch.unsqueeze(torch.linspace(0, 200, 201), dim=1) \n", "y=torch.from_numpy(data.reshape(len(data),1).astype('float32'))\n", "\n", "# torch can only train on Variable, so convert them to Variable\n", "x, y = Variable(x), Variable(y)\n", "\n", "# view data\n", "plt.figure(figsize=(10,4))\n", "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "plt.title('Regression Analysis')\n", "plt.xlabel('Independent variable')\n", "plt.ylabel('Dependent variable')\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "a3cf7c10", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["With a Gaussian shape, depending on how you count, there are 2 or 3 inflection points (i.e. kinks), so a 4 layer network (with 3 ReLU functions in between) with not very many parameters should be more than adequate. Let's try!"]}, {"cell_type": "code", "execution_count": null, "id": "7186c0e6", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell06\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "def makePlot(x,y,prediction,ax,fig,images,t,loss,ymin,ymax):\n", "    # plot and show learning process\n", "    plt.cla()\n", "    ax.set_title('Regression Analysis', fontsize=35)\n", "    ax.set_xlabel('Independent variable', fontsize=24)\n", "    ax.set_ylabel('Dependent variable', fontsize=24)\n", "    ax.set_ylim(ymin,ymax)\n", "    ax.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "    ax.plot(x.data.numpy(), prediction.data.numpy(), 'g-', lw=3)\n", "    ax.text(125, 16, 'Epoch = %d' % t, fontdict={'size': 24, 'color':  'red'})\n", "    ax.text(125, 14, 'Loss = %.4f' % loss.data.numpy(),fontdict={'size': 24, 'color':  'red'}) \n", "    fig.canvas.draw()       # draw the canvas, cache the renderer\n", "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n", "    image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n", "    images.append(image)\n", "\n", "\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 10),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(10, 10),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(10, 10),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(10, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "images=train(x,y,net,loss_func,optimizer,300,5,20)\n", "imageio.mimsave('data/L15/curve_gaus.gif', images, fps=10)\n", "Image(open('data/L15/curve_gaus.gif','rb').read())"]}, {"cell_type": "markdown", "id": "a01d2091", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["This result looks surprisingly poor. Maybe we need another ReLU function and more epochs?"]}, {"cell_type": "code", "execution_count": null, "id": "5b2a149b", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell07\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "n_hidden=10\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "images=train(x,y,net,loss_func,optimizer,1000,5,20)\n", "imageio.mimsave('data/L15/curve_gaus.gif', images, fps=10)\n", "Image(open('data/L15/curve_gaus.gif','rb').read())"]}, {"cell_type": "markdown", "id": "e87e7c02", "metadata": {"tags": ["learner", "md", "lect_05"]}, "source": ["Hmm... still not very impressive. How about increasing the number of parameters in the hidden layers? Give it a try.\n", "\n", "Training for these relatively simple functions should illustrate both the benefits and negative aspects of deep learning. This again just follows from the fact that deep learning is not really performing full fits and statistical tests. It's sacrificing the uncertainty and optimized tuning for larger scale generality, really more parameters. \n", "\n", "The following exercises will explore some of these issues in more detail. "]}, {"cell_type": "markdown", "id": "e2adffe8", "metadata": {"tags": ["learner", "md"]}, "source": ["<a name='exercises_15_5'></a>     \n", "\n", "| [Top](#section_15_0) | [Restart Section](#section_15_5) | [Next Section](#section_15_6) |\n"]}, {"cell_type": "markdown", "id": "e570f505", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.5.1</span>\n", "\n", "Consider the following neural network. How many hidden layers are there in this model? Enter your answer as an integer.\n", "\n", "<pre>\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "</pre>\n"]}, {"cell_type": "markdown", "id": "d64bf7a4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.5.2</span>\n", "\n", "Run the code cell shown below multiple times, **varying the number of hidden layers each time.** Which of the following statements describes the effect of increasing the number of hidden layers? Select all that apply:\n", "\n", "A) Increasing the number of hidden layers always leads to better classification performance.\\\n", "B) Increasing the number of hidden layers can help the model learn more complex patterns and improve classification performance, but may also increase the risk of overfitting.\\\n", "C) Increasing the number of hidden layers increases computational load and should only be done if better performance is needed.\\\n", "D) Increasing the number of hidden layers is only beneficial if the number of neurons in each layer is also increased.\n", "\n", "**Extra:** Try also varying the number of parameters within the hidden layers."]}, {"cell_type": "code", "execution_count": null, "id": "f6a7bb49", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L15.5.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def train(x,y,net,loss_func,opt,nepochs,ymin,ymax):\n", "    images = []\n", "    fig, ax = plt.subplots(figsize=(12,7))\n", "    for epoch in range(nepochs):\n", "        if epoch % 200 == 0: \n", "            print(\"epoch:\",epoch)\n", "        prediction = net(x)\n", "        loss = loss_func(prediction, y) \n", "        opt.zero_grad()\n", "        loss.backward() \n", "        optimizer.step()\n", "        # Minimize plots for faster running\n", "        if epoch == nepochs-1:\n", "            makePlot(x,y,prediction,ax,fig,images,epoch,loss,ymin,ymax)\n", "    return images\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "n_hidden=30\n", "\n", "#VARY THE NUMBER OF HIDDEN LAYERS BELOW\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "images=train(x,y,net,loss_func,optimizer,1000,5,20)\n", "imageio.mimsave('data/L15/curve_gaus.gif', images, fps=10)\n", "Image(open('data/L15/curve_gaus.gif','rb').read())\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}